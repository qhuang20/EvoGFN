{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import gzip\n",
    "import heapq\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dev = [torch.device('cpu')]\n",
    "# tf = lambda x: torch.FloatTensor(x).to(_dev[0])\n",
    "# tl = lambda x: torch.LongTensor(x).to(_dev[0])\n",
    "tf = lambda x: torch.FloatTensor(np.array(x)).to(_dev[0])  # Convert to numpy array first\n",
    "tl = lambda x: torch.LongTensor(np.array(x)).to(_dev[0])\n",
    "\n",
    "def set_device(dev):\n",
    "    _dev[0] = dev \n",
    "\n",
    "def func_corners(x):\n",
    "    ax = abs(x)\n",
    "    return (ax > 0.5).prod(-1) * 0.5 + ((ax < 0.8) * (ax > 0.6)).prod(-1) * 2 + 1e-1\n",
    "\n",
    "\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the dynamical system for the n-node system with sigmoid\n",
    "def node_system_with_sigmoid(x, t, coord):\n",
    "    matrix_dim = len(x)\n",
    "    M_tilde = np.reshape(coord, (matrix_dim, matrix_dim))\n",
    "    z = M_tilde.dot(x)  # Compute M_tilde * x\n",
    "    sigmoid_z = sigmoid(z)\n",
    "    dxdt = sigmoid_z - x  # Compute the derivative   \n",
    "    return dxdt\n",
    "\n",
    "# Calculate reward given the weights\n",
    "def reward_oscillator(coord, ndim):\n",
    "    delta = 0.0001  # 0.0001\n",
    "    matrix_dim = int(np.sqrt(ndim))\n",
    "    x0 = np.linspace(0, 1, matrix_dim, endpoint=False)  # Initial conditions\n",
    "    t = np.linspace(0, 20, 200)  # Define the time points\n",
    "    sol = odeint(node_system_with_sigmoid, x0, t, args=(coord, ))\n",
    "    \n",
    "    # Calculate the total number of sharp peaks across all time series\n",
    "    total_peaks = 0\n",
    "    for i in range(matrix_dim):  # Loop through each time series x1, x2, ..., xn\n",
    "        x_i = sol[:, i]\n",
    "        dx_i = np.diff(x_i)  # First derivative approximation\n",
    "        peaks = 0\n",
    "        for j in range(1, len(dx_i)):\n",
    "            if dx_i[j-1] > 0 and dx_i[j] < 0:  # Detect a peak\n",
    "                sharpness = x_i[j] - (x_i[j-1] + x_i[j+1]) / 2\n",
    "                if sharpness > delta:  # Check if the peak is sharp\n",
    "                    peaks += 1\n",
    "        total_peaks += peaks  # Add the number of sharp peaks for this time series\n",
    "    \n",
    "    if total_peaks == 0:\n",
    "        return 2.5e-5  # see log_reg_c\n",
    "    else:\n",
    "        return total_peaks**3  # number of peaks   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GridEnv:\n",
    "\n",
    "    def __init__(self, horizon, ndim=2, multiplier=2, func=None):\n",
    "        self.horizon = horizon\n",
    "        self.ndim = ndim\n",
    "        self.multiplier = multiplier\n",
    "        self.func = func   # Sets the reward function.\n",
    "        self._true_density = None\n",
    " \n",
    "    def obs(self, s=None):\n",
    "        \"\"\"\n",
    "        Returns a one-hot encoded observation of the current state.\n",
    "        The observation is a flattened vector representing the agent's position in the grid.\n",
    "        \"\"\"\n",
    "        s = np.int32(self._state if s is None else s)\n",
    "        z = np.zeros((self.horizon * self.ndim), dtype=np.float32)\n",
    "        z[np.arange(len(s)) * self.horizon + s] = 1 \n",
    "        return z    # one-hot agent's current position in the grid.\n",
    "    \n",
    "    def s2x(self, s):\n",
    "        \"\"\"\n",
    "        Transform the grid of indices (state s) to spherical coordinates and then calculate the cartesian coordinates\n",
    "        (x_1, x_2, ..., x_n) for n dimensions.\n",
    "        \n",
    "        s[0]: radial distance (r)\n",
    "        s[1:n-1]: polar angles (theta_1, theta_2, ..., theta_{n-2})\n",
    "        s[n-1]: azimuthal angle (phi)\n",
    "        \n",
    "        multiplier: scalar value to multiply the state s (default is 1)\n",
    "        \"\"\"\n",
    "        # Apply the multiplier to the state s\n",
    "        s = s * self.multiplier\n",
    "        \n",
    "        # Initialize the radius (r) from the first component of the state vector\n",
    "        r = s[0]\n",
    "    \n",
    "        # Initialize an array to hold the Cartesian coordinates\n",
    "        x = np.zeros(self.ndim)\n",
    "    \n",
    "        # Constants or parameters (assuming self.horizon refers to the number of steps in the grid)\n",
    "        horizon = self.horizon\n",
    "        \n",
    "        # Calculate the spherical to Cartesian conversion\n",
    "        product = r\n",
    "        for i in range(1, self.ndim):\n",
    "            if i == self.ndim - 1:\n",
    "                # The last angle (phi) ranges from 0 to 2π\n",
    "                phi = s[i] * 2 * np.pi / horizon\n",
    "                x[i - 1] = product * np.cos(phi)\n",
    "                x[i] = product * np.sin(phi)\n",
    "            else:\n",
    "                # The other angles (theta) range from 0 to π\n",
    "                theta = s[i] * np.pi / horizon\n",
    "                x[i - 1] = product * np.sin(theta)\n",
    "                product *= np.cos(theta)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state.\n",
    "        \"\"\"\n",
    "        self._state = np.int32([0] * self.ndim)   # start position (0,0...)\n",
    "        self._step = 0\n",
    "        return self.obs(), self.func(self.s2x(self._state), self.ndim), self._state\n",
    "\n",
    "    def parent_transitions(self, s, used_stop_action):\n",
    "        \"\"\"\n",
    "        Determines the parent states and corresponding actions that could have led to the current state.\n",
    "        \n",
    "        Parameters:\n",
    "        - s: The current state.\n",
    "        - used_stop_action: A boolean indicating if the stop action was used.\n",
    "        \n",
    "        Returns:\n",
    "        - A list of possible parent states (one-hot encoded).\n",
    "        - A list of corresponding actions.\n",
    "        \"\"\"\n",
    "        if used_stop_action:\n",
    "            return [self.obs(s)], [self.ndim]\n",
    "            \n",
    "        parents = []\n",
    "        actions = []\n",
    "        for i in range(self.ndim):\n",
    "            if s[i] > 0:\n",
    "                sp = s.copy()  # s + 0\n",
    "                sp[i] -= 1\n",
    "                if sp.max() == self.horizon - 1:  # Can't have a terminal parent\n",
    "                    continue\n",
    "                parents.append(self.obs(sp))  # Generate observation for parent state\n",
    "                actions.append(i)\n",
    "        return parents, actions\n",
    "\n",
    "    \n",
    "    def step(self, a):\n",
    "        \"\"\"\n",
    "        Updates the environment's state based on the action `a` and \n",
    "        returns the new observation, reward, done signal, and new state.\n",
    "        \"\"\"\n",
    "        s = self._state.copy()\n",
    "        if a < self.ndim:\n",
    "            s[a] += 1\n",
    "        \n",
    "        done = s.max() >= self.horizon - 1 or a == self.ndim\n",
    "        self._state = s  # Update the internal state\n",
    "        self._step += 1  # Increment step counter\n",
    "        \n",
    "        return self.obs(), 0 if not done else self.func(self.s2x(s), self.ndim), done, s\n",
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, args, env):\n",
    "        self.buf = []\n",
    "        self.strat = args.replay_strategy\n",
    "        self.sample_size = args.replay_sample_size\n",
    "        self.bufsize = args.replay_buf_size\n",
    "        self.env = env\n",
    "\n",
    "    def add(self, x, r_x):\n",
    "        if self.strat == 'top_k':\n",
    "            if len(self.buf) < self.bufsize or r_x > self.buf[0][0]:\n",
    "                self.buf = sorted(self.buf + [(r_x, x)])[-self.bufsize:]\n",
    "\n",
    "    def sample(self):\n",
    "        if not len(self.buf):\n",
    "            return []\n",
    "        idxs = np.random.randint(0, len(self.buf), self.sample_size)\n",
    "        return sum([self.generate_backward(*self.buf[i]) for i in idxs], [])  # Samples from the buffer and generates trajectories backward.\n",
    "\n",
    "    def generate_backward(self, r, s0):\n",
    "        s = np.int8(s0)\n",
    "        os0 = self.env.obs(s)\n",
    "        # If s0 is a forced-terminal state, the the action that leads\n",
    "        # to it is s0.argmax() which .parents finds, but if it isn't,\n",
    "        # we must indicate that the agent ended the trajectory with\n",
    "        # the stop action\n",
    "        used_stop_action = s.max() < self.env.horizon - 1\n",
    "        done = True\n",
    "        # Now we work backward from that last transition\n",
    "        traj = []\n",
    "        while s.sum() > 0:\n",
    "            parents, actions = self.env.parent_transitions(s, used_stop_action)\n",
    "            # add the transition\n",
    "            traj.append([tf(i) for i in (parents, actions, [r], [self.env.obs(s)], [done])])\n",
    "            # Then randomly choose a parent state\n",
    "            if not used_stop_action:\n",
    "                i = np.random.randint(0, len(parents))\n",
    "                a = actions[i]\n",
    "                s[a] -= 1\n",
    "            # Values for intermediary trajectory states:\n",
    "            used_stop_action = False\n",
    "            done = False\n",
    "            r = 0\n",
    "        return traj  # Generates a trajectory by working backward from a terminal state.\n",
    "\n",
    "def make_mlp(l, act=nn.LeakyReLU(), tail=[]):\n",
    "    return nn.Sequential(*(sum(\n",
    "        [[nn.Linear(i, o)] + ([act] if n < len(l)-2 else [])\n",
    "         for n, (i, o) in enumerate(zip(l, l[1:]))], []) + tail))\n",
    "    \n",
    "class FlowNetAgent:\n",
    "    def __init__(self, args, envs):\n",
    "        self.model = make_mlp([args.horizon * args.ndim] +\n",
    "                              [args.n_hid] * args.n_layers +\n",
    "                              [args.ndim+1])\n",
    "        self.model.to(args.dev)\n",
    "        self.target = copy.deepcopy(self.model)\n",
    "        self.envs = envs\n",
    "        self.ndim = args.ndim\n",
    "        self.tau = args.bootstrap_tau\n",
    "        self.replay = ReplayBuffer(args, envs[0])\n",
    "        self.log_reg_c = args.log_reg_c\n",
    "        \n",
    "        # to store training data\n",
    "        self.trn_all_losses = []\n",
    "        self.trn_all_visited_done = []\n",
    "\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.model.parameters()\n",
    "\n",
    "    def sample_many(self, mbsize, all_visited_done):\n",
    "        \"\"\"Collects transition data from multiple parallel trajectories.\"\"\"\n",
    "        batch = []  # store transitions.\n",
    "        batch += self.replay.sample()\n",
    "        s = tf([i.reset()[0] for i in self.envs])\n",
    "        done = [False] * mbsize\n",
    "        while not all(done):\n",
    "            # Note to self: this is ugly, ugly code\n",
    "            with torch.no_grad():\n",
    "                acts = Categorical(logits=self.model(s)).sample()   # Samples actions based on model's logits.\n",
    "            step = [i.step(a) for i,a in zip([e for d, e in zip(done, self.envs) if not d], acts)]\n",
    "            p_a = [self.envs[0].parent_transitions(sp_state, a == self.ndim)\n",
    "                   for a, (sp, r, done, sp_state) in zip(acts, step)]\n",
    "            batch += [[tf(i) for i in (p, a, [r], [sp], [d])]\n",
    "                      for (p, a), (sp, r, d, _) in zip(p_a, step)]\n",
    "            c = count(0)\n",
    "            m = {j:next(c) for j in range(mbsize) if not done[j]}\n",
    "            done = [bool(d or step[m[i]][2]) for i, d in enumerate(done)]\n",
    "            s = tf([i[0] for i in step if not i[2]])\n",
    "            for (_, r, d, sp) in step:\n",
    "                if d:\n",
    "                    all_visited_done.append((tuple(sp), r))  # (state, reward) pairs\n",
    "                    self.replay.add(tuple(sp), r) \n",
    "        return batch  # it returns a batch of collected transitions for training. {parents, actions, reward, next_state, done}\n",
    "\n",
    "    def sample_one_traj(self):\n",
    "        \"\"\"\n",
    "        Samples a single trajectory and returns it.\n",
    "        \"\"\"\n",
    "        traj = []\n",
    "        env = self.envs[0]\n",
    "        traj.append([[], [], 0, np.int32([0] * self.ndim), False])\n",
    "        \n",
    "        s = tf(env.reset()[0])\n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(s.unsqueeze(0))\n",
    "                action_dist = Categorical(logits=logits)\n",
    "                a = action_dist.sample().item()\n",
    "            sp, r, done_flag, sp_state = env.step(a)\n",
    "            parent_states, parent_actions = env.parent_transitions(sp_state, a == self.ndim)\n",
    "            traj.append([parent_states, parent_actions, r, sp_state, done_flag])\n",
    "            \n",
    "            s = tf(sp)\n",
    "            done = done_flag\n",
    "        return traj\n",
    "\n",
    "    def learn_from(self, it, batch):\n",
    "        loginf = tf([1000])\n",
    "        batch_idxs = tl(sum([[i]*len(parents) for i, (parents,_,_,_,_) in enumerate(batch)], []))\n",
    "        parents, actions, r, sp, done = map(torch.cat, zip(*batch))\n",
    "        parents_Qsa = self.model(parents)[torch.arange(parents.shape[0]), actions.long()]\n",
    "        in_flow = torch.log(self.log_reg_c + torch.zeros((sp.shape[0],))\n",
    "                            .index_add_(0, batch_idxs, torch.exp(parents_Qsa)))\n",
    "        if self.tau > 0:\n",
    "            with torch.no_grad(): next_q = self.target(sp)\n",
    "        else:\n",
    "            next_q = self.model(sp)\n",
    "        next_qd = next_q * (1-done).unsqueeze(1) + done.unsqueeze(1) * (-loginf)\n",
    "        out_flow = torch.logsumexp(torch.cat([torch.log(self.log_reg_c + r)[:, None], next_qd], 1), 1)\n",
    "        \n",
    "        term_loss = ((in_flow - out_flow) * done).pow(2).sum() / (done.sum() + 1e-20)\n",
    "        flow_loss = ((in_flow - out_flow) * (1-done)).pow(2).sum() / ((1-done).sum() + 1e-20)\n",
    "        \n",
    "        # loss = (in_flow - out_flow).pow(2).mean()\n",
    "        leaf_coef = 10\n",
    "        loss = term_loss * leaf_coef + flow_loss\n",
    "\n",
    "        if self.tau > 0:\n",
    "            for a,b in zip(self.model.parameters(), self.target.parameters()):\n",
    "                b.data.mul_(1-self.tau).add_(self.tau*a)\n",
    "\n",
    "        return loss, term_loss, flow_loss    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "\n",
    "def make_opt(params, args):\n",
    "    params = list(params)\n",
    "    if not len(params):\n",
    "        return None\n",
    "    if args.opt == 'adam':\n",
    "        opt = torch.optim.Adam(params, args.learning_rate,\n",
    "                               betas=(args.adam_beta1, args.adam_beta2))\n",
    "    elif args.opt == 'msgd':\n",
    "        opt = torch.optim.SGD(params, args.learning_rate, momentum=args.momentum)\n",
    "    return opt\n",
    "\n",
    "def compute_empirical_reward_distribution(visited):\n",
    "    if not len(visited):\n",
    "        return {}\n",
    "    reward_hist = defaultdict(int)\n",
    "    for _, reward in visited:\n",
    "        reward_hist[reward] += 1\n",
    "    total_visits = sum(reward_hist.values())\n",
    "    empirical_distribution = {reward: count / total_visits for reward, count in reward_hist.items()}\n",
    "    return empirical_distribution\n",
    "\n",
    "\n",
    "\n",
    "all_losses = []\n",
    "all_visited_done = []\n",
    "\n",
    "def main(args):\n",
    "    args.dev = torch.device(args.device)\n",
    "    set_device(args.dev)\n",
    "    f = {'default': None,\n",
    "         'corners': func_corners,\n",
    "         'oscillator': reward_oscillator,\n",
    "    }[args.func]\n",
    "    \n",
    "    env = GridEnv(args.horizon, args.ndim, multiplier=args.multiplier, func=f)\n",
    "    envs = [GridEnv(args.horizon, args.ndim, multiplier=args.multiplier, func=f)\n",
    "            for i in range(args.mbsize)] \n",
    "    ndim = args.ndim\n",
    "    nnode = args.nnode\n",
    "\n",
    "    if args.method == 'flownet':\n",
    "        agent = FlowNetAgent(args, envs)\n",
    "    elif args.method == 'mcmc':\n",
    "        agent = MHAgent(args, envs)\n",
    "    elif args.method == 'random_traj':\n",
    "        agent = RandomTrajAgent(args, envs)\n",
    "\n",
    "    opt = make_opt(agent.parameters(), args)\n",
    "\n",
    "        \n",
    "    \n",
    "    # Log file setup\n",
    "    root = args.save_path \n",
    "    log_file_path = os.path.join(root, f'trn-out-{args.nnode}-node.log')\n",
    "    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "    with open(log_file_path, 'w') as log_file:\n",
    "    \n",
    "        # Training Loop Setup\n",
    "        \n",
    "        ttsr = max(int(args.train_to_sample_ratio), 1) # train to sample ratio\n",
    "        sttr = max(int(1/args.train_to_sample_ratio), 1) # sample to train ratio\n",
    "        \n",
    "        for i in tqdm(range(args.n_train_steps+1), disable=not args.progress):\n",
    "            data = []  # a list of transitions from a batch of trajectories\n",
    "            for j in range(sttr):\n",
    "                \"\"\"Agent samples trajectories for training.\"\"\"\n",
    "                data += agent.sample_many(args.mbsize, all_visited_done)   \n",
    "            for j in range(ttsr):\n",
    "                \"\"\"Agent updates its model using the sampled data.\"\"\"\n",
    "                losses = agent.learn_from(i * ttsr + j, data) # returns (opt loss, *metrics)\n",
    "                if losses is not None:\n",
    "                    losses[0].backward(retain_graph=(not i % 50))\n",
    "                    if args.clip_grad_norm > 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(agent.parameters(),\n",
    "                                                       args.clip_grad_norm)\n",
    "                    opt.step()\n",
    "                    opt.zero_grad()\n",
    "                    all_losses.append([i.item() for i in losses])\n",
    "        \n",
    "            # Log empirical reward every 100 iterations\n",
    "            if not i % 100:\n",
    "                empirical_distribution = compute_empirical_reward_distribution(all_visited_done[-args.num_empirical_loss:])\n",
    "                print('Partial Empirical Reward Distribution:', empirical_distribution)\n",
    "                log_file.write(f'Partial Empirical Reward Distribution: {empirical_distribution}\\n')\n",
    "                log_file.flush()  # Ensure data is written to the log file\n",
    "                        \n",
    "            # Save the agent and model every 1000 iterations\n",
    "            if not i % 1000:\n",
    "                # Update agent with current all_losses and all_visited_done\n",
    "                agent.trn_all_losses = all_losses.copy()\n",
    "                agent.trn_all_visited_done = all_visited_done.copy()\n",
    "                \n",
    "                # Save the entire agent\n",
    "                agent_save_path = os.path.join(root, f\"agent_checkpoint_{i}.pkl.gz\")  \n",
    "                with gzip.open(agent_save_path, 'wb') as f: \n",
    "                    pickle.dump(agent, f)\n",
    "                print(f\"Agent checkpoint saved at iteration {i} in {nnode}.\")\n",
    "                log_file.write(f\"Agent checkpoint saved at iteration {i} in {nnode}.\\n\")\n",
    "                log_file.flush() \n",
    "            \n",
    "                # Save the agent's model separately\n",
    "                model_save_path = os.path.join(root, f\"model_checkpoint_{i}.pkl.gz\") \n",
    "                with gzip.open(model_save_path, 'wb') as f:  \n",
    "                    pickle.dump(agent.model, f)\n",
    "                print(f\"Model checkpoint saved at iteration {i} in {nnode}.\")\n",
    "                log_file.write(f\"Model checkpoint saved at iteration {i} in {nnode}.\\n\")\n",
    "                log_file.flush() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/90001 [00:00<12:50:12,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 1.0}\n",
      "Agent checkpoint saved at iteration 0 in 7.\n",
      "Model checkpoint saved at iteration 0 in 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 101/90001 [01:27<11:59:54,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.8923267326732673, 1: 0.10643564356435643, 8: 0.0012376237623762376}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 201/90001 [02:21<15:45:04,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9067164179104478, 1: 0.09079601990049752, 8: 0.0024875621890547263}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 301/90001 [03:36<17:25:48,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9136212624584718, 1: 0.08388704318936877, 8: 0.0024916943521594683}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 401/90001 [04:40<16:39:35,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9223815461346634, 1: 0.07543640897755612, 8: 0.0021820448877805485}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 501/90001 [05:41<16:14:13,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9301397205588823, 1: 0.06786427145708583, 8: 0.001996007984031936}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 601/90001 [06:29<8:45:41,  2.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9363560732113144, 1: 0.06198003327787022, 8: 0.0016638935108153079}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 701/90001 [07:05<13:15:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.942225392296719, 1: 0.056348074179743225, 8: 0.0014265335235378032}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 801/90001 [07:50<11:54:43,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9464731585518102, 1: 0.0516541822721598, 8: 0.0018726591760299626}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 901/90001 [08:35<12:05:35,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9504716981132075, 1: 0.047863485016648166, 8: 0.001664816870144284}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1000/90001 [09:22<10:08:30,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.954045954045954, 1: 0.044455544455544456, 8: 0.0014985014985014985}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1001/90001 [09:24<20:33:25,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent checkpoint saved at iteration 1000 in 7.\n",
      "Model checkpoint saved at iteration 1000 in 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1101/90001 [10:05<8:30:50,  2.90it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9564032697547684, 1: 0.04212079927338783, 8: 0.0014759309718437783}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1201/90001 [10:45<8:24:41,  2.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.958368026644463, 1: 0.040070774354704415, 8: 0.0014571190674437969, 27: 0.00010407993338884263}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1301/90001 [11:26<10:32:19,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9591660261337432, 1: 0.03920061491160646, 8: 0.0014411990776325902, 27: 0.0001921598770176787}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1401/90001 [12:08<9:12:38,  2.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9596716630977873, 1: 0.03854389721627409, 8: 0.001516773733047823, 27: 0.0002676659528907923}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1501/90001 [12:47<14:28:13,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9601932045303131, 1: 0.03772485009993338, 8: 0.0018321119253830779, 27: 0.0002498334443704197}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1602/90001 [13:21<7:11:14,  3.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9626795752654591, 1: 0.035368519675203, 8: 0.001717676452217364, 27: 0.00023422860712054967}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1701/90001 [13:55<7:38:57,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9648736037624926, 1: 0.033289241622574954, 8: 0.0016166960611405056, 27: 0.0002204585537918871}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1801/90001 [14:36<12:17:34,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9665463631315936, 1: 0.03171848972792893, 8: 0.001526929483620211, 27: 0.0002082176568573015}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1901/90001 [15:16<11:12:45,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9679116254602841, 1: 0.030444502893214098, 8: 0.0014466070489216202, 27: 0.00019726459758022093}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2000/90001 [15:51<7:08:28,  3.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9694527736131934, 1: 0.028985507246376812, 8: 0.001374312843578211, 27: 0.0001874062968515742}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2001/90001 [15:54<24:46:37,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent checkpoint saved at iteration 2000 in 7.\n",
      "Model checkpoint saved at iteration 2000 in 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2101/90001 [16:29<7:07:38,  3.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9700142789148025, 1: 0.02849833412660638, 8: 0.0013089005235602095, 27: 0.00017848643503093765}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2201/90001 [17:05<9:19:27,  2.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9698432530667879, 1: 0.02862335302135393, 8: 0.0013062244434348023, 27: 0.00022716946842344388}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2301/90001 [17:39<7:09:45,  3.40it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9711538461538461, 1: 0.027379400260756193, 8: 0.0012494567579313341, 27: 0.000217296827466319}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2401/90001 [18:13<8:33:16,  2.84it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9723032069970845, 1: 0.02629112869637651, 8: 0.001197417742607247, 27: 0.00020824656393169514}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2501/90001 [18:50<7:24:30,  3.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9734106357457017, 1: 0.025239904038384647, 8: 0.0011495401839264295, 27: 0.00019992003198720512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2601/90001 [19:24<8:56:13,  2.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9744329104190695, 1: 0.02426951172625913, 8: 0.0011053440984236831, 27: 0.00019223375624759708}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2701/90001 [19:59<8:35:49,  2.82it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9753794890781192, 1: 0.023370973713439467, 8: 0.0010644205849685302, 27: 0.00018511662347278786}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2801/90001 [20:33<9:13:42,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.976258479114602, 1: 0.022536594073545163, 8: 0.001026419136022849, 27: 0.0001785076758300607}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2901/90001 [21:07<8:49:42,  2.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9770768700448121, 1: 0.02175973802137194, 8: 0.0009910375732506032, 27: 0.0001723543605653223}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3000/90001 [21:41<6:39:36,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.97784071976008, 1: 0.02103465511496168, 8: 0.0009580139953348884, 27: 0.00016661112962345885}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3001/90001 [21:44<28:54:40,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent checkpoint saved at iteration 3000 in 7.\n",
      "Model checkpoint saved at iteration 3000 in 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3101/90001 [22:18<7:05:08,  3.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9773863269912931, 1: 0.02128345694937117, 8: 0.0010883585940019349, 27: 0.0002418574653337633}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3201/90001 [23:01<9:23:02,  2.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9780927835051546, 1: 0.020618556701030927, 8: 0.0010543580131208998, 27: 0.00023430178069353328}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3301/90001 [23:36<7:20:12,  3.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9787564374431991, 1: 0.019993941229930322, 8: 0.0010224174492578007, 27: 0.0002272038776128446}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3401/90001 [24:11<7:58:52,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9793810643928257, 1: 0.019406057042046457, 8: 0.000992355189650103, 27: 0.00022052337547780066}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3502/90001 [24:45<7:31:51,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9799700085689803, 1: 0.018851756640959727, 8: 0.0009640102827763496, 27: 0.00021422450728363326}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3601/90001 [25:19<8:55:48,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9805262427103583, 1: 0.018328242154956955, 8: 0.000937239655651208, 27: 0.00020827547903360177}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3701/90001 [25:57<10:42:10,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9810524182653337, 1: 0.01783301810321535, 8: 0.0009119156984598757, 27: 0.00020264793299108348}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3801/90001 [26:38<9:14:18,  2.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9815180215732702, 1: 0.017396737700605104, 8: 0.000887924230465667, 27: 0.0001973164956590371}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3901/90001 [27:14<7:29:26,  3.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9819917969751346, 1: 0.016950781850807485, 8: 0.0008651627787746732, 27: 0.00019225839528326071}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4000/90001 [27:48<8:35:17,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9823794051487128, 1: 0.01658960259935016, 8: 0.0008435391152211947, 27: 0.00018745313671582103}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4001/90001 [27:52<35:24:36,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent checkpoint saved at iteration 4000 in 7.\n",
      "Model checkpoint saved at iteration 4000 in 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4101/90001 [28:31<8:08:31,  2.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9826871494757377, 1: 0.016276517922457937, 8: 0.000853450377956596, 27: 0.000182882223847842}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4201/90001 [29:07<8:27:24,  2.82it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9830695072601762, 1: 0.015918828850273743, 8: 0.0008331349678647941, 27: 0.00017852892168531301}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4301/90001 [29:43<8:38:19,  2.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9832597070448733, 1: 0.015752150662636598, 8: 0.0008137642408742153, 27: 0.00017437805161590327}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4401/90001 [30:12<5:59:15,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.983498068620768, 1: 0.01550783912747103, 8: 0.0007952738014087707, 27: 0.00019881845035219268}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4501/90001 [30:43<8:31:27,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9838091535214397, 1: 0.015218840257720507, 8: 0.0007776049766718507, 27: 0.00019440124416796267}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4601/90001 [31:18<9:17:46,  2.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9840252119104542, 1: 0.01502390784612041, 8: 0.0007607041947402739, 27: 0.00019017604868506847}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4701/90001 [31:53<6:39:55,  3.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9838598170602, 1: 0.015182939800042544, 8: 0.000771112529249096, 27: 0.00018613061050840247}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4802/90001 [32:26<5:46:09,  4.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9836492397417205, 1: 0.015413455530097896, 8: 0.000755051031035201, 27: 0.00018225369714642783}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4901/90001 [32:58<6:50:11,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9838553356457865, 1: 0.015226484390940624, 8: 0.0007396449704142012, 27: 0.0001785349928586003}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5000/90001 [33:27<6:33:46,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9841531693661267, 1: 0.014947010597880424, 8: 0.0007248550289942012, 27: 0.00017496500699860028}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5001/90001 [33:32<37:53:30,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent checkpoint saved at iteration 5000 in 7.\n",
      "Model checkpoint saved at iteration 5000 in 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5101/90001 [34:03<7:13:15,  3.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9844638306214468, 1: 0.014653989413840423, 8: 0.0007106449715742012, 27: 0.00017153499313860026}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5201/90001 [34:38<7:27:33,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9847625456642953, 1: 0.014372236108440685, 8: 0.0006969813497404346, 27: 0.00016823687752355316}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5301/90001 [35:10<6:53:36,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9850499905678174, 1: 0.014101112997547632, 8: 0.0006838332390115072, 27: 0.00016506319562346728}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5401/90001 [35:43<7:00:50,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9851879281614516, 1: 0.013955748935382336, 8: 0.000694315867431957, 27: 0.0001620070357341233}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5501/90001 [36:16<8:12:08,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9854117433193965, 1: 0.013747500454462824, 8: 0.0006816942374113798, 27: 0.00015906198872932194}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5601/90001 [36:51<7:59:53,  2.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9854043920728441, 1: 0.013747545081235494, 8: 0.0006918407427245135, 27: 0.0001562221031958579}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 5701/90001 [37:23<6:49:27,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9856384844764077, 1: 0.01352832836344501, 8: 0.0006797053148570426, 27: 0.00015348184529029996}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 5801/90001 [37:56<8:45:39,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.9857352180658507, 1: 0.013424409584554388, 8: 0.0006895362868470954, 27: 0.0001508360627478021}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 5901/90001 [38:31<8:20:11,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Empirical Reward Distribution: {2.5e-05: 0.985510930350788, 1: 0.013578207083545162, 8: 0.0007413997627520759, 27: 0.0001694628029147602}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 5976/90001 [38:55<9:07:21,  2.56it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m args \u001b[38;5;241m=\u001b[39m Args()\n\u001b[1;32m     37\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_num_threads(\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 69\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sttr):\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Agent samples trajectories for training.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_many\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmbsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_visited_done\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ttsr):\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Agent updates its model using the sampled data.\"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 186\u001b[0m, in \u001b[0;36mFlowNetAgent.sample_many\u001b[0;34m(self, mbsize, all_visited_done)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Collects transition data from multiple parallel trajectories.\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m batch \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# store transitions.\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m s \u001b[38;5;241m=\u001b[39m tf([i\u001b[38;5;241m.\u001b[39mreset()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs])\n\u001b[1;32m    188\u001b[0m done \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m] \u001b[38;5;241m*\u001b[39m mbsize\n",
      "Cell \u001b[0;32mIn[29], line 129\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    128\u001b[0m idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_size)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_backward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idxs], [])\n",
      "Cell \u001b[0;32mIn[29], line 129\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    128\u001b[0m idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_size)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idxs], [])\n",
      "Cell \u001b[0;32mIn[29], line 145\u001b[0m, in \u001b[0;36mReplayBuffer.generate_backward\u001b[0;34m(self, r, s0)\u001b[0m\n\u001b[1;32m    143\u001b[0m parents, actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mparent_transitions(s, used_stop_action)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# add the transition\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m traj\u001b[38;5;241m.\u001b[39mappend([tf(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (parents, actions, [r], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mobs(s)], [done])])\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Then randomly choose a parent state\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m used_stop_action:\n",
      "Cell \u001b[0;32mIn[29], line 145\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    143\u001b[0m parents, actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mparent_transitions(s, used_stop_action)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# add the transition\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m traj\u001b[38;5;241m.\u001b[39mappend([\u001b[43mtf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (parents, actions, [r], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mobs(s)], [done])])\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Then randomly choose a parent state\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m used_stop_action:\n",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m _dev \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# tf = lambda x: torch.FloatTensor(x).to(_dev[0])\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# tl = lambda x: torch.LongTensor(x).to(_dev[0])\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(_dev[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Convert to numpy array first\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mLongTensor(np\u001b[38;5;241m.\u001b[39marray(x))\u001b[38;5;241m.\u001b[39mto(_dev[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_device\u001b[39m(dev):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    save_path = 'results-v3/7-node-v1'\n",
    "    device = 'cpu'\n",
    "    progress = True  \n",
    "    method = 'flownet'\n",
    "    learning_rate = 5e-4\n",
    "    opt = 'adam'\n",
    "    adam_beta1 = 0.9\n",
    "    adam_beta2 = 0.999\n",
    "    momentum = 0.9  # SGD with momentum\n",
    "    mbsize = 8  # number of parallel environments (trajectories) are collected by one agent (One Agent's model is shared in Many Environments). \n",
    "    train_to_sample_ratio = 1.0  # determines how many times the agent should update its model (train) for each set of data it collects from the environment. \n",
    "    clip_grad_norm = 0.\n",
    "    n_hid = 256  # number of hidden units in each hidden layer\n",
    "    n_layers = 3\n",
    "    n_train_steps = 90000 \n",
    "    num_empirical_loss = 200000  # number of samples used to compute the empirical distribution loss during evaluation.\n",
    "    \n",
    "    \n",
    "    # Env\n",
    "    func = 'oscillator'\n",
    "    horizon = 12  # 4*3\n",
    "    nnode = 7\n",
    "    ndim = nnode*nnode \n",
    "    multiplier = 2\n",
    "    \n",
    "    # Flownet\n",
    "    bootstrap_tau = 0.0  # no bootstrapping,target network isn't being updated gradually but possibly replaced entirely at some point.\n",
    "    replay_strategy = 'top_k'  # 'top_k' or 'none'\n",
    "    replay_sample_size = 5  # number of experiences to sample from the replay buffer at each update step.\n",
    "    replay_buf_size = 100  #  size of the replay buffer, which stores past experiences for the agent to learn from.\n",
    "    log_reg_c = 2.5e-5\n",
    "\n",
    "\n",
    "\n",
    "args = Args()\n",
    "torch.set_num_threads(200)\n",
    "main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming all_losses, all_visited, empirical_distrib_losses are lists of lists or arrays\n",
    "\n",
    "# Plot the losses\n",
    "if all_losses:\n",
    "    all_losses_array = np.array(all_losses)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(all_losses_array.shape[1]):\n",
    "        plt.plot(all_losses_array[:, i], label=f'Loss {i+1}')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Losses Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort visited states by reward in descending order\n",
    "visited_sorted_by_reward = sorted(all_visited_done, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Sample the top 10 states with the highest rewards\n",
    "top_states = visited_sorted_by_reward[:20]\n",
    "\n",
    "print(\"Top 10 States with Highest Rewards:\")\n",
    "for i, (state, reward) in enumerate(top_states):\n",
    "    print(f\"State {i+1}: {state}, Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "args = Args()\n",
    "\n",
    "env = GridEnv(args.horizon, args.ndim, func=f)\n",
    "\n",
    "agent_path = \"/home/dannyhuang/gflownet/frontline/results-v3/7-node/agent_checkpoint_30000.pkl.gz\"\n",
    "print(agent_path)\n",
    "\n",
    "# Load the trained_agent object from the file\n",
    "try:\n",
    "    with gzip.open(agent_path, 'rb') as f:\n",
    "        trained_agent = pickle.load(f)\n",
    "    print(trained_agent)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {agent_path} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "delta = 0.0001\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the dynamical system for the n-node system with sigmoid\n",
    "def n_node_system_with_sigmoid(x, t, *weights):\n",
    "    nnode = int(np.sqrt(len(weights)))\n",
    "    M_tilde = np.array(weights).reshape((nnode, nnode))\n",
    "    \n",
    "    # Compute M_tilde * x\n",
    "    z = M_tilde.dot(x)\n",
    "    \n",
    "    # Apply sigmoid\n",
    "    sigmoid_z = sigmoid(z)\n",
    "    \n",
    "    # Compute the derivative\n",
    "    dxdt = sigmoid_z - x\n",
    "    return dxdt\n",
    "\n",
    "# Function to update plot and calculate reward\n",
    "def update_plot(*weights):\n",
    "    nnode = int(np.sqrt(len(weights)))\n",
    "    ndim = nnode * nnode\n",
    "\n",
    "    # Initial conditions\n",
    "    x0 = np.linspace(0, 1, nnode, endpoint=False)\n",
    "\n",
    "    # Define the time points\n",
    "    t = np.linspace(0, 20, 200)\n",
    "\n",
    "    # Solve ODE\n",
    "    sol = odeint(n_node_system_with_sigmoid, x0, t, args=tuple(weights))\n",
    "    \n",
    "    # Clear current plot\n",
    "    plt.clf()\n",
    "    \n",
    "    # Plot the results\n",
    "    for i in range(nnode):\n",
    "        plt.plot(t, sol[:, i], label=f'$x_{i+1}$')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Concentration')\n",
    "    plt.title(f'{nnode}-Node System Dynamics with Sigmoid')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate reward based on the number of sharp peaks\n",
    "    reward = calculate_reward(sol)\n",
    "    print(f\"Reward: {reward}\")\n",
    "\n",
    "# Function to calculate reward based on the number of sharp peaks\n",
    "def calculate_reward(sol, delta=delta):\n",
    "    reward = 0\n",
    "    for i in range(sol.shape[1]):  # Loop through each time series x1, x2, ..., xn\n",
    "        x_i = sol[:, i]\n",
    "        dx_i = np.diff(x_i)  # First derivative approximation\n",
    "        peaks = 0\n",
    "        for j in range(1, len(dx_i)):\n",
    "            if dx_i[j-1] > 0 and dx_i[j] < 0:  # Detect a peak\n",
    "                sharpness = x_i[j] - (x_i[j-1] + x_i[j+1]) / 2\n",
    "                if sharpness > delta:  # Check if the peak is sharp\n",
    "                    peaks += 1\n",
    "        reward += peaks  # Add the number of sharp peaks as reward\n",
    "    return reward\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# nnode = 5  # Change this to set the number of nodes\n",
    "# ndim = nnode * nnode\n",
    "# test_weights = np.ones(ndim)\n",
    "# update_plot(*test_weights)\n",
    "\n",
    "# test_state = [3, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "# 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "# test_weight = env.s2x(np.int32(test_state))\n",
    "# update_plot(*test_weight)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create sliders for each weight\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "\n",
    "# Initialize weights based on the test_state\n",
    "initial_weights = env.s2x(np.int32([3, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))\n",
    "\n",
    "# Create sliders for all 49 weights\n",
    "sliders = [widgets.FloatSlider(value=initial_weights[i], min=-15, max=15, step=0.1, description=f'Weight {i+1}') for i in range(49)]\n",
    "\n",
    "# Create an output widget to display the plot\n",
    "out = widgets.Output()\n",
    "\n",
    "# Define the update function\n",
    "def update_plot_with_sliders(**w):\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        new_weights = list(w.values())\n",
    "        update_plot(*new_weights)\n",
    "\n",
    "# Create the interactive plot\n",
    "interactive_plot = interactive(update_plot_with_sliders, **{f'w{i}': slider for i, slider in enumerate(sliders)})\n",
    "\n",
    "# Display the sliders and the plot\n",
    "display(widgets.VBox([interactive_plot, out]))\n",
    "\n",
    "# Initial plot update\n",
    "update_plot_with_sliders(**{f'w{i}': initial_weights[i] for i in range(49)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
